{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=T\tTP=329\tFN=18\n",
      "label=F\tFP=66\tTN=142\n",
      "Bayesian accuracy = 0.8486486486486486\n"
     ]
    }
   ],
   "source": [
    "# bayesian classifier\n",
    "\n",
    "import typing as t\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import math\n",
    "import mailbox\n",
    "import random\n",
    "\n",
    "# get a list of tokens from sequence\n",
    "def normalize_text(text: str)-> t.Sequence[str]:\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# parse mbox file\n",
    "def parse_mbox_body(path_mbox: str)-> t.List[str]:\n",
    "    contents = []\n",
    "    mb_spam = mailbox.mbox(path_mbox)\n",
    "    for message in mb_spam.itervalues():\n",
    "        body = None\n",
    "        if message.is_multipart():\n",
    "            for part in message.walk():\n",
    "                if part.is_multipart():\n",
    "                    for subpart in part.walk():\n",
    "                        if subpart.get_content_type() == 'text/plain':\n",
    "                            body = subpart.get_payload(decode=True)\n",
    "                elif part.get_content_type() == 'text/plain':\n",
    "                    body = part.get_payload(decode=True)\n",
    "        elif message.get_content_type() == 'text/plain':\n",
    "            body = message.get_payload(decode=True)\n",
    "        content = \"\"\n",
    "        if body is not None:\n",
    "            content = re.sub(\"<[^>]+>\", \"\", body.decode('unicode_escape'))\n",
    "        contents.append(content)\n",
    "        \n",
    "    return contents\n",
    "\n",
    "\n",
    "# get a list of tokens from sequence\n",
    "def load_emails(path_mbox: str)-> t.List[t.Dict[str, int]]:\n",
    "    emails = []\n",
    "    contents = parse_mbox_body(path_mbox)\n",
    "    for content in contents:\n",
    "        tokens = normalize_text(content)\n",
    "        \n",
    "        token2count = {}\n",
    "        for token in tokens:\n",
    "            token2count[token] = 1\n",
    "        emails.append(token2count)\n",
    "    return emails\n",
    "\n",
    "\n",
    "# return P(word|C) in category C\n",
    "def log_prob_t_C(texts: t.Sequence[t.Dict[str,int]]) -> t.Dict[str, float]:\n",
    "    cat_freq = {}\n",
    "    for text in texts:\n",
    "        for token in text.keys():\n",
    "            if token not in cat_freq:\n",
    "                cat_freq[token] = 0\n",
    "            cat_freq[token] += 1  \n",
    "    \n",
    "    cat_prob = {}\n",
    "    for token, count in cat_freq.items():\n",
    "        # always add 1 + freq to make the distribution more smooth\n",
    "        p = 1.0 * (1 + count) / (1 + len(texts))\n",
    "        cat_prob[token] = math.log10(p)\n",
    "    \n",
    "    return cat_prob\n",
    "\n",
    "# evaluate log_P(t1,t2,t3,...|C) token sequence\n",
    "def evaluate_prob(\n",
    "    tokens: t.Sequence[str], # t1, t2, t3, ...\n",
    "    log_prob_t_C: t.Dict[str,float], # log_P(t1|C)\n",
    "    N: int, # corpus size\n",
    ") -> float:\n",
    "    log_prob_sequence = 0\n",
    "    for token in tokens:\n",
    "        if token in log_prob_t_C:\n",
    "            log_prob_sequence += log_prob_t_C[token]\n",
    "        else:\n",
    "            log_prob_sequence += math.log10(1.0/(N+1))\n",
    "    return log_prob_sequence\n",
    "\n",
    "# return 1 if Positive, otherwise 0 if Negative\n",
    "def classify_bayesian(\n",
    "    tokens, # list of str tokens\n",
    "    log_prob_t_pos, \n",
    "    N_pos,\n",
    "    log_prob_t_neg,\n",
    "    N_neg,\n",
    "):\n",
    "    # log_P(t1,t2,t3, ... | postive)\n",
    "    log_prob_seq_pos = evaluate_prob(tokens, log_prob_t_pos, N_pos)\n",
    "    # log_P(t1,t2,t3, ... | negative)\n",
    "    log_prob_seq_neg = evaluate_prob(tokens, log_prob_t_neg, N_neg)\n",
    "    # log_P(positive) \n",
    "    log_prob_pos = math.log10(1.0*N_pos/(N_pos+N_neg))\n",
    "    # log_P(positive) \n",
    "    log_prob_neg = math.log10(1.0*N_neg/(N_pos+N_neg))\n",
    "    \n",
    "    # P(prob_seq_pos) * P(prob_pos)\n",
    "    score_pos = log_prob_seq_pos + log_prob_pos\n",
    "    # P(prob_seq_neg) * P(prob_neg)\n",
    "    score_neg = log_prob_seq_neg + log_prob_neg\n",
    "    \n",
    "    return 1 if score_pos > score_neg else 0\n",
    "    \n",
    "\n",
    "# label = 1\n",
    "texts_pos = load_emails(\"promotion.mbox\")\n",
    "\n",
    "# label = 0\n",
    "texts_neg = load_emails(\"update.mbox\")\n",
    "\n",
    "texts_pos_train = []\n",
    "texts_pos_eval = []\n",
    "texts_neg_train = []\n",
    "texts_neg_eval = []\n",
    "\n",
    "for t in texts_pos: \n",
    "    if random.random() < 0.7:\n",
    "        texts_pos_train.append(t)\n",
    "    else:\n",
    "        texts_pos_eval.append(t)\n",
    "\n",
    "for t in texts_neg: \n",
    "    if random.random() < 0.7:\n",
    "        texts_neg_train.append(t)\n",
    "    else:\n",
    "        texts_neg_eval.append(t)\n",
    "        \n",
    "\n",
    "log_prob_t_pos = log_prob_t_C(texts_pos_train)\n",
    "log_prob_t_neg = log_prob_t_C(texts_neg_train)\n",
    "\n",
    "# true positive\n",
    "TP = 0\n",
    "# true negative\n",
    "TN = 0\n",
    "# false positive\n",
    "FP = 0\n",
    "# false negative\n",
    "FN = 0\n",
    "\n",
    "for t in texts_pos_eval:\n",
    "    y = classify_bayesian(\n",
    "        t.keys(),\n",
    "        log_prob_t_pos, \n",
    "        len(texts_pos_train),\n",
    "        log_prob_t_neg, \n",
    "        len(texts_neg_train))\n",
    "    if y == 1:\n",
    "        TP += 1\n",
    "    else:\n",
    "        FN += 1\n",
    "\n",
    "for t in texts_neg_eval:\n",
    "    y = classify_bayesian(\n",
    "        t.keys(),\n",
    "        log_prob_t_pos, \n",
    "        len(texts_pos_train),\n",
    "        log_prob_t_neg, \n",
    "        len(texts_neg_train))\n",
    "    if y == 1:\n",
    "        FP += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "\n",
    "print (f\"label=T\\tTP={TP}\\tFN={FN}\")\n",
    "print (f\"label=F\\tFP={FP}\\tTN={TN}\")\n",
    "print (f\"Bayesian accuracy = {1.0*(TP+TN)/(TP+TN+FP+FN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy = 0.8126126126126126\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "def select_words(\n",
    "    log_prob_t_pos, \n",
    "    N_pos,\n",
    "    log_prob_t_neg,\n",
    "    N_neg,\n",
    "    N_select\n",
    "):\n",
    "    # select most discriminative positive words\n",
    "    tuples_pos = []\n",
    "    for token, prob_pos in log_prob_t_pos.items():\n",
    "        if token in log_prob_t_neg:\n",
    "            prob_neg = log_prob_t_neg[token]\n",
    "        else:\n",
    "            prob_neg = math.log10(1.0/(1 + N_neg))\n",
    "        tuples_pos.append((token, prob_pos-prob_neg))\n",
    "    \n",
    "    tuples_pos = sorted(tuples_pos, key = lambda x: float(x[1]), reverse = True)\n",
    "   \n",
    "    # select most discriminative negative words\n",
    "    tuples_neg = []\n",
    "    for token, prob_neg in log_prob_t_neg.items():\n",
    "        if token in log_prob_t_pos:\n",
    "            prob_pos = log_prob_t_pos[token]\n",
    "        else:\n",
    "            prob_pos = math.log10(1.0/(1 + N_pos))\n",
    "        tuples_neg.append((token, prob_neg-prob_pos))\n",
    "        \n",
    "    tuples_neg = sorted(tuples_neg, key = lambda x: float(x[1]), reverse = True)\n",
    "    words = {}\n",
    "    index = 0\n",
    "    for x in (tuples_pos[:N_select] + tuples_neg[:N_select]):\n",
    "        words[x[0]] = index\n",
    "        index += 1\n",
    "    return words\n",
    "\n",
    "topN = 20\n",
    "words = select_words(\n",
    "        log_prob_t_pos, \n",
    "        len(texts_pos_train),\n",
    "        log_prob_t_neg, \n",
    "        len(texts_neg_train),\n",
    "        topN\n",
    ")\n",
    "\n",
    "def gen_feature(words, texts):\n",
    "    X = []\n",
    "    for text in texts:\n",
    "        features = [0]*len(words)\n",
    "        for token in text:\n",
    "            if token in words:\n",
    "                features[words[token]] = 1\n",
    "        features.append(1)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "#numpy.ones\n",
    "\n",
    "X_train = gen_feature(words, texts_pos_train + texts_neg_train)\n",
    "Y_train = np.expand_dims(np.append(np.ones(len(texts_pos_train)), np.zeros(len(texts_neg_train))), axis=1)\n",
    "X = np.append(X_train, Y_train, axis=1)\n",
    "np.random.shuffle(X)\n",
    "X_train = X[:,:X.shape[1]-1]\n",
    "Y_train = X[:,X.shape[1]-1:].squeeze()\n",
    "\n",
    "X_eval = gen_feature(words, texts_pos_eval + texts_neg_eval)\n",
    "Y_eval = np.expand_dims(np.append(np.ones(len(texts_pos_eval)), np.zeros(len(texts_neg_eval))), axis=1)\n",
    "X = np.append(X_eval, Y_eval, axis=1)\n",
    "np.random.shuffle(X)\n",
    "X_eval = X[:,:X.shape[1]-1]\n",
    "Y_eval = X[:,X.shape[1]-1:].squeeze()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, Y_train)\n",
    "score =clf.score(X_eval, Y_eval)\n",
    "print(f\"LR accuracy = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
